{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e344d2bd",
   "metadata": {},
   "source": [
    "# 1. Data Colection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958c482",
   "metadata": {},
   "source": [
    "### Import necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f58f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022e1f7",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402c19bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>L7_PROTO</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>OUT_BYTES</th>\n",
       "      <th>OUT_PKTS</th>\n",
       "      <th>TCP_FLAGS</th>\n",
       "      <th>CLIENT_TCP_FLAGS</th>\n",
       "      <th>...</th>\n",
       "      <th>TCP_WIN_MAX_IN</th>\n",
       "      <th>TCP_WIN_MAX_OUT</th>\n",
       "      <th>ICMP_TYPE</th>\n",
       "      <th>ICMP_IPV4_TYPE</th>\n",
       "      <th>DNS_QUERY_ID</th>\n",
       "      <th>DNS_QUERY_TYPE</th>\n",
       "      <th>DNS_TTL_ANSWER</th>\n",
       "      <th>FTP_COMMAND_RET_CODE</th>\n",
       "      <th>Label</th>\n",
       "      <th>Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2017</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4762</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32534</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>512</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-13423</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>188.0</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-28756</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>188.0</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   L4_SRC_PORT  L4_DST_PORT  PROTOCOL  L7_PROTO  IN_BYTES  IN_PKTS  OUT_BYTES  \\\n",
       "0        -2017           80         6       7.0       140        1          0   \n",
       "1        -4762           80         6       7.0       280        2          0   \n",
       "2        32534           80         6       7.0       280        2          0   \n",
       "3       -13423           80        17     188.0        56        2          0   \n",
       "4       -28756           80        17     188.0        56        2          0   \n",
       "\n",
       "   OUT_PKTS  TCP_FLAGS  CLIENT_TCP_FLAGS  ...  TCP_WIN_MAX_IN  \\\n",
       "0         0          2                 2  ...             512   \n",
       "1         0          2                 2  ...             512   \n",
       "2         0          2                 2  ...             512   \n",
       "3         0          0                 0  ...               0   \n",
       "4         0          0                 0  ...               0   \n",
       "\n",
       "   TCP_WIN_MAX_OUT  ICMP_TYPE  ICMP_IPV4_TYPE  DNS_QUERY_ID  DNS_QUERY_TYPE  \\\n",
       "0                0          0               0             0               0   \n",
       "1                0          0               0             0               0   \n",
       "2                0          0               0             0               0   \n",
       "3                0          0               0             0               0   \n",
       "4                0          0               0             0               0   \n",
       "\n",
       "   DNS_TTL_ANSWER  FTP_COMMAND_RET_CODE  Label  Attack  \n",
       "0               0                   0.0      1     DoS  \n",
       "1               0                   0.0      1     DoS  \n",
       "2               0                   0.0      1     DoS  \n",
       "3               0                   0.0      1    DDoS  \n",
       "4               0                   0.0      1    DDoS  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../dataset/NF_BOT_IoT_V2/NF-BoT-IoT-V2.parquet\", engine=\"pyarrow\")  # or engine=\"fastparquet\"\n",
    "# View the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705ceb9",
   "metadata": {},
   "source": [
    "# 2. Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4b2b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: overflow encountered in square\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: overflow encountered in square\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>L7_PROTO</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>OUT_BYTES</th>\n",
       "      <th>OUT_PKTS</th>\n",
       "      <th>TCP_FLAGS</th>\n",
       "      <th>CLIENT_TCP_FLAGS</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM_PKTS_1024_TO_1514_BYTES</th>\n",
       "      <th>TCP_WIN_MAX_IN</th>\n",
       "      <th>TCP_WIN_MAX_OUT</th>\n",
       "      <th>ICMP_TYPE</th>\n",
       "      <th>ICMP_IPV4_TYPE</th>\n",
       "      <th>DNS_QUERY_ID</th>\n",
       "      <th>DNS_QUERY_TYPE</th>\n",
       "      <th>DNS_TTL_ANSWER</th>\n",
       "      <th>FTP_COMMAND_RET_CODE</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "      <td>3.042009e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-9.345863e+02</td>\n",
       "      <td>1.081121e+02</td>\n",
       "      <td>1.116985e+01</td>\n",
       "      <td>9.175757e+01</td>\n",
       "      <td>6.441983e+02</td>\n",
       "      <td>2.840693e+00</td>\n",
       "      <td>2.521820e+02</td>\n",
       "      <td>3.510150e-01</td>\n",
       "      <td>3.665827e+00</td>\n",
       "      <td>1.332763e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.744911e-01</td>\n",
       "      <td>4.787784e+02</td>\n",
       "      <td>9.461602e+02</td>\n",
       "      <td>5.286983e+03</td>\n",
       "      <td>2.065228e+01</td>\n",
       "      <td>7.279295e+00</td>\n",
       "      <td>3.086579e-03</td>\n",
       "      <td>6.950239e+00</td>\n",
       "      <td>5.693245e-03</td>\n",
       "      <td>9.957450e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.884213e+04</td>\n",
       "      <td>3.835130e+03</td>\n",
       "      <td>5.490616e+00</td>\n",
       "      <td>9.266840e+01</td>\n",
       "      <td>1.141003e+05</td>\n",
       "      <td>1.084273e+02</td>\n",
       "      <td>1.092240e+05</td>\n",
       "      <td>6.997525e+01</td>\n",
       "      <td>7.234989e+00</td>\n",
       "      <td>2.416236e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.956851e+01</td>\n",
       "      <td>2.325950e+03</td>\n",
       "      <td>5.147257e+03</td>\n",
       "      <td>1.266814e+04</td>\n",
       "      <td>4.948491e+01</td>\n",
       "      <td>5.639778e+02</td>\n",
       "      <td>2.649712e-01</td>\n",
       "      <td>1.391326e+03</td>\n",
       "      <td>1.591469e+00</td>\n",
       "      <td>6.509132e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.276800e+04</td>\n",
       "      <td>-3.276800e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-5.221800e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.723100e+04</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>5.600000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.710000e+03</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>1.120000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.541100e+04</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.880000e+02</td>\n",
       "      <td>2.800000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.276700e+04</td>\n",
       "      <td>3.276700e+04</td>\n",
       "      <td>5.800000e+01</td>\n",
       "      <td>2.480000e+02</td>\n",
       "      <td>1.890273e+08</td>\n",
       "      <td>4.915300e+04</td>\n",
       "      <td>1.969787e+08</td>\n",
       "      <td>3.786400e+04</td>\n",
       "      <td>2.140000e+02</td>\n",
       "      <td>1.980000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.583800e+04</td>\n",
       "      <td>6.553500e+04</td>\n",
       "      <td>4.240800e+04</td>\n",
       "      <td>6.528000e+04</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>6.553400e+04</td>\n",
       "      <td>2.550000e+02</td>\n",
       "      <td>5.184000e+05</td>\n",
       "      <td>5.300000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        L4_SRC_PORT   L4_DST_PORT      PROTOCOL      L7_PROTO      IN_BYTES  \\\n",
       "count  3.042009e+07  3.042009e+07  3.042009e+07  3.042009e+07  3.042009e+07   \n",
       "mean  -9.345863e+02  1.081121e+02  1.116985e+01  9.175757e+01  6.441983e+02   \n",
       "std    1.884213e+04  3.835130e+03  5.490616e+00  9.266840e+01  1.141003e+05   \n",
       "min   -3.276800e+04 -3.276800e+04  1.000000e+00  0.000000e+00  4.000000e+00   \n",
       "25%   -1.723100e+04  8.000000e+01  6.000000e+00  7.000000e+00  5.600000e+01   \n",
       "50%   -1.710000e+03  8.000000e+01  6.000000e+00  7.000000e+00  1.120000e+02   \n",
       "75%    1.541100e+04  8.000000e+01  1.700000e+01  1.880000e+02  2.800000e+02   \n",
       "max    3.276700e+04  3.276700e+04  5.800000e+01  2.480000e+02  1.890273e+08   \n",
       "\n",
       "            IN_PKTS     OUT_BYTES      OUT_PKTS     TCP_FLAGS  \\\n",
       "count  3.042009e+07  3.042009e+07  3.042009e+07  3.042009e+07   \n",
       "mean   2.840693e+00  2.521820e+02  3.510150e-01  3.665827e+00   \n",
       "std    1.084273e+02  1.092240e+05  6.997525e+01  7.234989e+00   \n",
       "min    1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    2.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    2.000000e+00  0.000000e+00  0.000000e+00  2.000000e+00   \n",
       "75%    3.000000e+00  0.000000e+00  0.000000e+00  2.000000e+00   \n",
       "max    4.915300e+04  1.969787e+08  3.786400e+04  2.140000e+02   \n",
       "\n",
       "       CLIENT_TCP_FLAGS  ...  NUM_PKTS_1024_TO_1514_BYTES  TCP_WIN_MAX_IN  \\\n",
       "count      3.042009e+07  ...                 3.042009e+07    3.042009e+07   \n",
       "mean       1.332763e+00  ...                 2.744911e-01    4.787784e+02   \n",
       "std        2.416236e+00  ...                 6.956851e+01    2.325950e+03   \n",
       "min        0.000000e+00  ...                 0.000000e+00    0.000000e+00   \n",
       "25%        0.000000e+00  ...                 0.000000e+00    0.000000e+00   \n",
       "50%        2.000000e+00  ...                 0.000000e+00    5.120000e+02   \n",
       "75%        2.000000e+00  ...                 0.000000e+00    5.120000e+02   \n",
       "max        1.980000e+02  ...                 3.583800e+04    6.553500e+04   \n",
       "\n",
       "       TCP_WIN_MAX_OUT     ICMP_TYPE  ICMP_IPV4_TYPE  DNS_QUERY_ID  \\\n",
       "count     3.042009e+07  3.042009e+07    3.042009e+07  3.042009e+07   \n",
       "mean      9.461602e+02  5.286983e+03    2.065228e+01  7.279295e+00   \n",
       "std       5.147257e+03  1.266814e+04    4.948491e+01  5.639778e+02   \n",
       "min       0.000000e+00  0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "25%       0.000000e+00  0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "50%       0.000000e+00  0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "75%       0.000000e+00  0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "max       4.240800e+04  6.528000e+04    2.550000e+02  6.553400e+04   \n",
       "\n",
       "       DNS_QUERY_TYPE  DNS_TTL_ANSWER  FTP_COMMAND_RET_CODE         Label  \n",
       "count    3.042009e+07    3.042009e+07          3.042009e+07  3.042009e+07  \n",
       "mean     3.086579e-03    6.950239e+00          5.693245e-03  9.957450e-01  \n",
       "std      2.649712e-01    1.391326e+03          1.591469e+00  6.509132e-02  \n",
       "min      0.000000e+00   -5.221800e+04          0.000000e+00  0.000000e+00  \n",
       "25%      0.000000e+00    0.000000e+00          0.000000e+00  1.000000e+00  \n",
       "50%      0.000000e+00    0.000000e+00          0.000000e+00  1.000000e+00  \n",
       "75%      0.000000e+00    0.000000e+00          0.000000e+00  1.000000e+00  \n",
       "max      2.550000e+02    5.184000e+05          5.300000e+02  1.000000e+00  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6296b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30420086 entries, 0 to 30420085\n",
      "Data columns (total 43 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   L4_SRC_PORT                  int16  \n",
      " 1   L4_DST_PORT                  int16  \n",
      " 2   PROTOCOL                     int8   \n",
      " 3   L7_PROTO                     float32\n",
      " 4   IN_BYTES                     int32  \n",
      " 5   IN_PKTS                      int32  \n",
      " 6   OUT_BYTES                    int32  \n",
      " 7   OUT_PKTS                     int32  \n",
      " 8   TCP_FLAGS                    int16  \n",
      " 9   CLIENT_TCP_FLAGS             int16  \n",
      " 10  SERVER_TCP_FLAGS             int16  \n",
      " 11  FLOW_DURATION_MILLISECONDS   int32  \n",
      " 12  DURATION_IN                  int16  \n",
      " 13  DURATION_OUT                 int16  \n",
      " 14  MIN_TTL                      int16  \n",
      " 15  MAX_TTL                      int16  \n",
      " 16  LONGEST_FLOW_PKT             int32  \n",
      " 17  SHORTEST_FLOW_PKT            int16  \n",
      " 18  MIN_IP_PKT_LEN               int16  \n",
      " 19  MAX_IP_PKT_LEN               int32  \n",
      " 20  SRC_TO_DST_SECOND_BYTES      float32\n",
      " 21  DST_TO_SRC_SECOND_BYTES      float32\n",
      " 22  RETRANSMITTED_IN_BYTES       int32  \n",
      " 23  RETRANSMITTED_IN_PKTS        int16  \n",
      " 24  RETRANSMITTED_OUT_BYTES      int16  \n",
      " 25  RETRANSMITTED_OUT_PKTS       int8   \n",
      " 26  SRC_TO_DST_AVG_THROUGHPUT    int64  \n",
      " 27  DST_TO_SRC_AVG_THROUGHPUT    int64  \n",
      " 28  NUM_PKTS_UP_TO_128_BYTES     int16  \n",
      " 29  NUM_PKTS_128_TO_256_BYTES    int16  \n",
      " 30  NUM_PKTS_256_TO_512_BYTES    int16  \n",
      " 31  NUM_PKTS_512_TO_1024_BYTES   int32  \n",
      " 32  NUM_PKTS_1024_TO_1514_BYTES  int32  \n",
      " 33  TCP_WIN_MAX_IN               int32  \n",
      " 34  TCP_WIN_MAX_OUT              int32  \n",
      " 35  ICMP_TYPE                    int32  \n",
      " 36  ICMP_IPV4_TYPE               int16  \n",
      " 37  DNS_QUERY_ID                 int32  \n",
      " 38  DNS_QUERY_TYPE               int16  \n",
      " 39  DNS_TTL_ANSWER               int32  \n",
      " 40  FTP_COMMAND_RET_CODE         float32\n",
      " 41  Label                        int8   \n",
      " 42  Attack                       object \n",
      "dtypes: float32(4), int16(18), int32(15), int64(2), int8(3), object(1)\n",
      "memory usage: 3.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588f7127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30420086, 43)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe166c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['L4_SRC_PORT', 'L4_DST_PORT', 'PROTOCOL', 'L7_PROTO', 'IN_BYTES',\n",
       "       'IN_PKTS', 'OUT_BYTES', 'OUT_PKTS', 'TCP_FLAGS', 'CLIENT_TCP_FLAGS',\n",
       "       'SERVER_TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS', 'DURATION_IN',\n",
       "       'DURATION_OUT', 'MIN_TTL', 'MAX_TTL', 'LONGEST_FLOW_PKT',\n",
       "       'SHORTEST_FLOW_PKT', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN',\n",
       "       'SRC_TO_DST_SECOND_BYTES', 'DST_TO_SRC_SECOND_BYTES',\n",
       "       'RETRANSMITTED_IN_BYTES', 'RETRANSMITTED_IN_PKTS',\n",
       "       'RETRANSMITTED_OUT_BYTES', 'RETRANSMITTED_OUT_PKTS',\n",
       "       'SRC_TO_DST_AVG_THROUGHPUT', 'DST_TO_SRC_AVG_THROUGHPUT',\n",
       "       'NUM_PKTS_UP_TO_128_BYTES', 'NUM_PKTS_128_TO_256_BYTES',\n",
       "       'NUM_PKTS_256_TO_512_BYTES', 'NUM_PKTS_512_TO_1024_BYTES',\n",
       "       'NUM_PKTS_1024_TO_1514_BYTES', 'TCP_WIN_MAX_IN', 'TCP_WIN_MAX_OUT',\n",
       "       'ICMP_TYPE', 'ICMP_IPV4_TYPE', 'DNS_QUERY_ID', 'DNS_QUERY_TYPE',\n",
       "       'DNS_TTL_ANSWER', 'FTP_COMMAND_RET_CODE', 'Label', 'Attack'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b516e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5591783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset shape: (304201, 43)\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac=0.01, random_state=42)\n",
    "print(f\"Sampled dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86edf72f",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780cd12f",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783eb00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L4_SRC_PORT                    0\n",
       "L4_DST_PORT                    0\n",
       "PROTOCOL                       0\n",
       "L7_PROTO                       0\n",
       "IN_BYTES                       0\n",
       "IN_PKTS                        0\n",
       "OUT_BYTES                      0\n",
       "OUT_PKTS                       0\n",
       "TCP_FLAGS                      0\n",
       "CLIENT_TCP_FLAGS               0\n",
       "SERVER_TCP_FLAGS               0\n",
       "FLOW_DURATION_MILLISECONDS     0\n",
       "DURATION_IN                    0\n",
       "DURATION_OUT                   0\n",
       "MIN_TTL                        0\n",
       "MAX_TTL                        0\n",
       "LONGEST_FLOW_PKT               0\n",
       "SHORTEST_FLOW_PKT              0\n",
       "MIN_IP_PKT_LEN                 0\n",
       "MAX_IP_PKT_LEN                 0\n",
       "SRC_TO_DST_SECOND_BYTES        0\n",
       "DST_TO_SRC_SECOND_BYTES        0\n",
       "RETRANSMITTED_IN_BYTES         0\n",
       "RETRANSMITTED_IN_PKTS          0\n",
       "RETRANSMITTED_OUT_BYTES        0\n",
       "RETRANSMITTED_OUT_PKTS         0\n",
       "SRC_TO_DST_AVG_THROUGHPUT      0\n",
       "DST_TO_SRC_AVG_THROUGHPUT      0\n",
       "NUM_PKTS_UP_TO_128_BYTES       0\n",
       "NUM_PKTS_128_TO_256_BYTES      0\n",
       "NUM_PKTS_256_TO_512_BYTES      0\n",
       "NUM_PKTS_512_TO_1024_BYTES     0\n",
       "NUM_PKTS_1024_TO_1514_BYTES    0\n",
       "TCP_WIN_MAX_IN                 0\n",
       "TCP_WIN_MAX_OUT                0\n",
       "ICMP_TYPE                      0\n",
       "ICMP_IPV4_TYPE                 0\n",
       "DNS_QUERY_ID                   0\n",
       "DNS_QUERY_TYPE                 0\n",
       "DNS_TTL_ANSWER                 0\n",
       "FTP_COMMAND_RET_CODE           0\n",
       "Label                          0\n",
       "Attack                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values: fill numerical with mean, others with 0\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    else:\n",
    "        df[col] = df[col].fillna(0)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a0510",
   "metadata": {},
   "source": [
    "### Data Encoding and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "346e7ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features (PROTOCOL, L7_PROTO)\n",
    "le_protocol = LabelEncoder()\n",
    "df['PROTOCOL'] = le_protocol.fit_transform(df['PROTOCOL'].astype(str))\n",
    "le_l7_proto = LabelEncoder()\n",
    "df['L7_PROTO'] = le_l7_proto.fit_transform(df['L7_PROTO'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce6078",
   "metadata": {},
   "source": [
    "### Binary Classfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee5045c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract labels for binary classification (Label: 0=benign, 1=attack)\n",
    "y = df['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d405a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features (exclude Label, Attack)\n",
    "feature_cols = [col for col in df.columns if col not in ['Label', 'Attack']]\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11cce467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset shape: (304201, 42)\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessed DataFrame\n",
    "df_processed = pd.DataFrame(X, columns=feature_cols)\n",
    "df_processed['Label'] = y\n",
    "df_processed['L4_SRC_PORT'] = df['L4_SRC_PORT'].values  # Retain for graph\n",
    "df_processed['L4_DST_PORT'] = df['L4_DST_PORT'].values\n",
    "print(f\"Preprocessed dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8edf8b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_15848\\3125400933.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  x = torch.tensor(x, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph constructed: 64884 nodes, 608402 edges\n"
     ]
    }
   ],
   "source": [
    "# === Graph Construction ===\n",
    "# Map unique ports to node indices\n",
    "src_ports = df_processed['L4_SRC_PORT'].values\n",
    "dst_ports = df_processed['L4_DST_PORT'].values\n",
    "unique_ports = np.unique(np.concatenate([src_ports, dst_ports]))\n",
    "port_to_idx = {port: idx for idx, port in enumerate(unique_ports)}\n",
    "\n",
    "# Aggregate node features and labels\n",
    "node_features = defaultdict(list)\n",
    "node_labels = defaultdict(list)\n",
    "for idx in range(len(df_processed)):\n",
    "    row = df_processed.iloc[idx]\n",
    "    src_port = row['L4_SRC_PORT']\n",
    "    dst_port = row['L4_DST_PORT']\n",
    "    features = row.drop(['Label', 'L4_SRC_PORT', 'L4_DST_PORT']).values\n",
    "    label = row['Label']\n",
    "    node_features[port_to_idx[src_port]].append(features[:-1])  # Exclude label\n",
    "    node_features[port_to_idx[dst_port]].append(features[:-1])\n",
    "    node_labels[port_to_idx[src_port]].append(label)\n",
    "    node_labels[port_to_idx[dst_port]].append(label)\n",
    "\n",
    "# Compute average features and labels per node\n",
    "x = []\n",
    "y = []\n",
    "for port_idx in range(len(unique_ports)):\n",
    "    if port_idx in node_features:\n",
    "        x.append(np.mean(node_features[port_idx], axis=0))\n",
    "        y.append(np.mean(node_labels[port_idx]))  # Majority for binary\n",
    "    else:\n",
    "        x.append(np.zeros(df_processed.shape[1] - 3))  # Default features\n",
    "        y.append(0)  # Default label\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Construct edges (bidirectional)\n",
    "edge_index = []\n",
    "for idx in range(len(df_processed)):\n",
    "    row = df_processed.iloc[idx]\n",
    "    src_idx = port_to_idx[row['L4_SRC_PORT']]\n",
    "    dst_idx = port_to_idx[row['L4_DST_PORT']]\n",
    "    edge_index.append([src_idx, dst_idx])\n",
    "    edge_index.append([dst_idx, src_idx])  # Bidirectional\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "print(f\"Graph constructed: {data.num_nodes} nodes, {data.num_edges} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1558b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6244\n",
      "Epoch 2, Loss: 0.4585\n",
      "Epoch 3, Loss: 0.3627\n",
      "Epoch 4, Loss: 0.2231\n",
      "Epoch 5, Loss: 0.1738\n",
      "Epoch 6, Loss: 0.1187\n",
      "Epoch 7, Loss: 0.1021\n",
      "Epoch 8, Loss: 0.1089\n",
      "Epoch 9, Loss: 0.1006\n",
      "Epoch 10, Loss: 0.0864\n",
      "Epoch 11, Loss: 0.0904\n",
      "Epoch 12, Loss: 0.0824\n",
      "Epoch 13, Loss: 0.0804\n",
      "Epoch 14, Loss: 0.0768\n",
      "Epoch 15, Loss: 0.0766\n",
      "Epoch 16, Loss: 0.0832\n",
      "Epoch 17, Loss: 0.0676\n",
      "Epoch 18, Loss: 0.0682\n",
      "Epoch 19, Loss: 0.0642\n",
      "Epoch 20, Loss: 0.0607\n",
      "Epoch 21, Loss: 0.0557\n",
      "Epoch 22, Loss: 0.0617\n",
      "Epoch 23, Loss: 0.0556\n",
      "Epoch 24, Loss: 0.0533\n",
      "Epoch 25, Loss: 0.0537\n",
      "Epoch 26, Loss: 0.0510\n",
      "Epoch 27, Loss: 0.0530\n",
      "Epoch 28, Loss: 0.0505\n",
      "Epoch 29, Loss: 0.0558\n",
      "Epoch 30, Loss: 0.0496\n",
      "Epoch 31, Loss: 0.0480\n",
      "Epoch 32, Loss: 0.0482\n",
      "Epoch 33, Loss: 0.0498\n",
      "Epoch 34, Loss: 0.0476\n",
      "Epoch 35, Loss: 0.0471\n",
      "Epoch 36, Loss: 0.0456\n",
      "Epoch 37, Loss: 0.0461\n",
      "Epoch 38, Loss: 0.0439\n",
      "Epoch 39, Loss: 0.0437\n",
      "Epoch 40, Loss: 0.0430\n",
      "Epoch 41, Loss: 0.0434\n",
      "Epoch 42, Loss: 0.0437\n",
      "Epoch 43, Loss: 0.0467\n",
      "Epoch 44, Loss: 0.0407\n",
      "Epoch 45, Loss: 0.0448\n",
      "Epoch 46, Loss: 0.0442\n",
      "Epoch 47, Loss: 0.0390\n",
      "Epoch 48, Loss: 0.0395\n",
      "Epoch 49, Loss: 0.0406\n",
      "Epoch 50, Loss: 0.0389\n",
      "Epoch 51, Loss: 0.0390\n",
      "Epoch 52, Loss: 0.0393\n",
      "Epoch 53, Loss: 0.0396\n",
      "Epoch 54, Loss: 0.0378\n",
      "Epoch 55, Loss: 0.0364\n",
      "Epoch 56, Loss: 0.0396\n",
      "Epoch 57, Loss: 0.0362\n",
      "Epoch 58, Loss: 0.0357\n",
      "Epoch 59, Loss: 0.0354\n",
      "Epoch 60, Loss: 0.0349\n",
      "Epoch 61, Loss: 0.0355\n",
      "Epoch 62, Loss: 0.0354\n",
      "Epoch 63, Loss: 0.0343\n",
      "Epoch 64, Loss: 0.0342\n",
      "Epoch 65, Loss: 0.0332\n",
      "Epoch 66, Loss: 0.0342\n",
      "Epoch 67, Loss: 0.0328\n",
      "Epoch 68, Loss: 0.0327\n",
      "Epoch 69, Loss: 0.0386\n",
      "Epoch 70, Loss: 0.0312\n",
      "Epoch 71, Loss: 0.0311\n",
      "Epoch 72, Loss: 0.0312\n",
      "Epoch 73, Loss: 0.0310\n",
      "Epoch 74, Loss: 0.0314\n",
      "Epoch 75, Loss: 0.0301\n",
      "Epoch 76, Loss: 0.0298\n",
      "Epoch 77, Loss: 0.0386\n",
      "Epoch 78, Loss: 0.0301\n",
      "Epoch 79, Loss: 0.0294\n",
      "Epoch 80, Loss: 0.0306\n",
      "Epoch 81, Loss: 0.0288\n",
      "Epoch 82, Loss: 0.0303\n",
      "Epoch 83, Loss: 0.0290\n",
      "Epoch 84, Loss: 0.0309\n",
      "Epoch 85, Loss: 0.0280\n",
      "Epoch 86, Loss: 0.0275\n",
      "Epoch 87, Loss: 0.0282\n",
      "Epoch 88, Loss: 0.0287\n",
      "Epoch 89, Loss: 0.0271\n",
      "Epoch 90, Loss: 0.0275\n",
      "Epoch 91, Loss: 0.0266\n",
      "Epoch 92, Loss: 0.0274\n",
      "Epoch 93, Loss: 0.0260\n",
      "Epoch 94, Loss: 0.0259\n",
      "Epoch 95, Loss: 0.0259\n",
      "Epoch 96, Loss: 0.0262\n",
      "Epoch 97, Loss: 0.0265\n",
      "Epoch 98, Loss: 0.0260\n",
      "Epoch 99, Loss: 0.0250\n",
      "Epoch 100, Loss: 0.0252\n",
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.9929\n",
      "Precision: 0.9936\n",
      "Recall: 0.9991\n",
      "F1: 0.9964\n"
     ]
    }
   ],
   "source": [
    "# === GNN Model Definition ===\n",
    "# Define GraphSAGE model as a class (required by PyTorch)\n",
    "class GNNNIDS(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNNNIDS, self).__init__()\n",
    "        self.conv1 = SAGEConv(data.num_features, 64)\n",
    "        self.conv2 = SAGEConv(64, 64)\n",
    "        self.fc = torch.nn.Linear(64, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = GNNNIDS()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# === Model Training ===\n",
    "# Create train/test masks (80% train, 20% test)\n",
    "num_nodes = data.num_nodes\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "indices = np.random.permutation(num_nodes)\n",
    "train_size = int(0.8 * num_nodes)\n",
    "train_mask[indices[:train_size]] = True\n",
    "test_mask[indices[train_size:]] = True\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Train model for 100 epochs\n",
    "model.train()\n",
    "train_losses = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    # if (epoch + 1) % 100 == 0:\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# === Step 5: Model Evaluation ===\n",
    "# Evaluate model \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    y_true = data.y[data.test_mask].numpy()\n",
    "    y_pred = pred[data.test_mask].numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # ROC curve\n",
    "    y_score = out[data.test_mask][:, 1].numpy()\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.savefig('training_loss.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d3ef5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 121680384 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 342\u001b[0m\n\u001b[0;32m    339\u001b[0m     plot_metrics(metrics)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 342\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[15], line 315\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    312\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNF-BoT-IoT-V2.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with actual path\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# Step 1: Preprocess data\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m df_processed, scaler, label_encoder \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(file_path)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Step 2: Construct graph\u001b[39;00m\n\u001b[0;32m    318\u001b[0m data \u001b[38;5;241m=\u001b[39m construct_graph(df_processed)\n",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m(file_path, sample_frac)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mLoad and preprocess the NF-BoT-IoT-v2 dataset for multi-class classification.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    label_encoder (LabelEncoder): Fitted encoder for Attack labels.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load dataset (sample to reduce memory usage)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv(file_path)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(file_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# or engine=\"fastparquet\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39msample_frac, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSampled dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1811\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1800\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   1801\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   1802\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1808\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1809\u001b[0m     )\n\u001b[1;32m-> 1811\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mread(columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39muse_threads,\n\u001b[0;32m   1812\u001b[0m                     use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1454\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1446\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1447\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1448\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   1449\u001b[0m         ]\n\u001b[0;32m   1450\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1451\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   1452\u001b[0m         )\n\u001b[1;32m-> 1454\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   1455\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_expression,\n\u001b[0;32m   1456\u001b[0m     use_threads\u001b[38;5;241m=\u001b[39muse_threads\n\u001b[0;32m   1457\u001b[0m )\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:562\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3804\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 121680384 failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def load_and_preprocess_data(file_path, sample_frac=0.03):\n",
    "    \"\"\"\n",
    "    Load and preprocess the NF-BoT-IoT-v2 dataset for multi-class classification.\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        sample_frac (float): Fraction of data to sample (default: 3%).\n",
    "    Returns:\n",
    "        df_processed (pd.DataFrame): Preprocessed dataset.\n",
    "        scaler (StandardScaler): Fitted scaler for numerical features.\n",
    "        label_encoder (LabelEncoder): Fitted encoder for Attack labels.\n",
    "    \"\"\"\n",
    "    # Load dataset (sample to reduce memory usage)\n",
    "    # df = pd.read_csv(file_path)\n",
    "    df = pd.read_parquet(file_path, engine=\"pyarrow\")  # or engine=\"fastparquet\"\n",
    "    df = df.sample(frac=sample_frac, random_state=42)\n",
    "    print(f\"Sampled dataset shape: {df.shape}\")\n",
    "\n",
    "    # Handle missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    # Encode categorical features (e.g., PROTOCOL, L7_PROTO)\n",
    "    categorical_cols = ['PROTOCOL', 'L7_PROTO']\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "    # Encode labels for multi-class classification (Attack column)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['Attack'])\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "    # Select numerical features for node features (exclude Label, Attack)\n",
    "    feature_cols = [col for col in df.columns if col not in ['Label', 'Attack']]\n",
    "    X = df[feature_cols].values\n",
    "\n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    df_processed = pd.DataFrame(X, columns=feature_cols)\n",
    "    df_processed['Attack'] = y\n",
    "    print(f\"Preprocessed dataset shape: {df_processed.shape}\")\n",
    "\n",
    "    return df_processed, scaler, label_encoder\n",
    "\n",
    "# Step 2: Graph Construction\n",
    "def construct_graph(df_processed):\n",
    "    \"\"\"\n",
    "    Construct a graph where nodes are source/destination ports, and edges represent flows.\n",
    "    Args:\n",
    "        df_processed (pd.DataFrame): Preprocessed dataset.\n",
    "    Returns:\n",
    "        data (torch_geometric.data.Data): Graph data object for PyG.\n",
    "    \"\"\"\n",
    "    # Map unique ports to node indices\n",
    "    src_ports = df_processed['L4_SRC_PORT'].values\n",
    "    dst_ports = df_processed['L4_DST_PORT'].values\n",
    "    unique_ports = np.unique(np.concatenate([src_ports, dst_ports]))\n",
    "    port_to_idx = {port: idx for idx, port in enumerate(unique_ports)}\n",
    "\n",
    "    # Node features: Aggregate flow features for each port\n",
    "    node_features = defaultdict(list)\n",
    "    node_labels = defaultdict(list)\n",
    "    for idx, row in df_processed.iterrows():\n",
    "        src_port = row['L4_SRC_PORT']\n",
    "        dst_port = row['L4_DST_PORT']\n",
    "        features = row.drop(['Attack', 'L4_SRC_PORT', 'L4_DST_PORT']).values\n",
    "        label = row['Attack']\n",
    "        \n",
    "        node_features[port_to_idx[src_port]].append(features[:-1])  # Exclude label\n",
    "        node_features[port_to_idx[dst_port]].append(features[:-1])\n",
    "        node_labels[port_to_idx[src_port]].append(label)\n",
    "        node_labels[port_to_idx[dst_port]].append(label)\n",
    "\n",
    "    # Average features and take mode for labels per node\n",
    "    x = []\n",
    "    y = []\n",
    "    for port_idx in range(len(unique_ports)):\n",
    "        if port_idx in node_features:\n",
    "            x.append(np.mean(node_features[port_idx], axis=0))\n",
    "            # For multi-class, take mode of labels\n",
    "            labels = node_labels[port_idx]\n",
    "            y.append(np.bincount(labels.astype(int)).argmax())\n",
    "        else:\n",
    "            x.append(np.zeros(df_processed.shape[1] - 3))  # Default features\n",
    "            y.append(0)  # Default label\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    # Construct edges (bidirectional)\n",
    "    edge_index = []\n",
    "    for idx, row in df_processed.iterrows():\n",
    "        src_idx = port_to_idx[row['L4_SRC_PORT']]\n",
    "        dst_idx = port_to_idx[row['L4_DST_PORT']]\n",
    "        edge_index.append([src_idx, dst_idx])\n",
    "        edge_index.append([dst_idx, src_idx])  # Bidirectional\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Create PyG Data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "    print(f\"Graph constructed: {data.num_nodes} nodes, {data.num_edges} edges\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Step 3: GNN Model Definition\n",
    "class GNNNIDS(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network model for NIDS using GraphSAGE.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNNIDS, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 4: Model Training\n",
    "def train_model(data, model, optimizer, epochs=100):\n",
    "    \"\"\"\n",
    "    Train the GNN model with training and validation metrics.\n",
    "    Args:\n",
    "        data (Data): Graph data object.\n",
    "        model (torch.nn.Module): GNN model.\n",
    "        optimizer: PyTorch optimizer.\n",
    "        epochs (int): Number of training epochs.\n",
    "    Returns:\n",
    "        metrics (dict): Training and validation loss/accuracy per epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Create train/val/test masks (70% train, 15% val, 15% test)\n",
    "    num_nodes = data.num_nodes\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    indices = np.random.permutation(num_nodes)\n",
    "    train_size = int(0.7 * num_nodes)\n",
    "    val_size = int(0.15 * num_nodes)\n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Training metrics\n",
    "        pred = out.argmax(dim=1)\n",
    "        train_acc = accuracy_score(data.y[data.train_mask].numpy(), pred[data.train_mask].numpy())\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "            val_pred = out.argmax(dim=1)\n",
    "            val_acc = accuracy_score(data.y[data.val_mask].numpy(), val_pred[data.val_mask].numpy())\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "def evaluate_model(data, model, num_classes, label_encoder):\n",
    "    \"\"\"\n",
    "    Evaluate the GNN model for multi-class classification.\n",
    "    Args:\n",
    "        data (Data): Graph data object.\n",
    "        model (torch.nn.Module): Trained GNN model.\n",
    "        num_classes (int): Number of classes.\n",
    "        label_encoder (LabelEncoder): Encoder for class labels.\n",
    "    Returns:\n",
    "        metrics (dict): Evaluation metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        y_true = data.y[data.test_mask].numpy()\n",
    "        y_pred = pred[data.test_mask].numpy()\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        }\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "        # ROC Curve (one-vs-rest for multi-class)\n",
    "        y_score = F.softmax(out[data.test_mask], dim=1).numpy()\n",
    "        y_true_bin = np.zeros((y_true.size, num_classes))\n",
    "        y_true_bin[np.arange(y_true.size), y_true] = 1\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red'])\n",
    "        for i, color in zip(range(num_classes), colors):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, color=color, label=f'{label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves (One-vs-Rest)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig('roc_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Step 6: Plot Training vs Validation Metrics\n",
    "def plot_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Plot training vs validation loss and accuracy.\n",
    "    Args:\n",
    "        metrics (dict): Training and validation metrics.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(metrics['train_losses']) + 1)\n",
    "\n",
    "    # Loss plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, metrics['train_losses'], label='Training Loss')\n",
    "    plt.plot(epochs, metrics['val_losses'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('train_val_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Accuracy plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, metrics['train_accuracies'], label='Training Accuracy')\n",
    "    plt.plot(epochs, metrics['val_accuracies'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('train_val_accuracy.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # File path to dataset\n",
    "    file_path = 'NF-BoT-IoT-V2.parquet'  # Replace with actual path\n",
    "\n",
    "    # Step 1: Preprocess data\n",
    "    df_processed, scaler, label_encoder = load_and_preprocess_data(file_path)\n",
    "\n",
    "    # Step 2: Construct graph\n",
    "    data = construct_graph(df_processed)\n",
    "\n",
    "    # Step 3: Initialize model\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    model = GNNNIDS(\n",
    "        in_channels=data.num_features,\n",
    "        hidden_channels=64,\n",
    "        out_channels=num_classes\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Step 4: Train model\n",
    "    metrics = train_model(data, model, optimizer)\n",
    "\n",
    "    # Step 5: Evaluate model\n",
    "    eval_metrics = evaluate_model(data, model, num_classes, label_encoder)\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for key, value in eval_metrics.items():\n",
    "        print(f\"{key.capitalize()}: {value:.4f}\")\n",
    "\n",
    "    # Step 6: Plot training vs validation metrics\n",
    "    plot_metrics(metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa881cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
