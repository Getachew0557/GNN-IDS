{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1340130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Load CICIDS2017 dataset (assuming CSV format)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove duplicates and handle missing values\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "    \n",
    "    # Select features (excluding non-numeric and label columns)\n",
    "    feature_cols = [col for col in df.columns if col != 'Label' and df[col].dtype != 'object']\n",
    "    X = df[feature_cols].values\n",
    "    y = df['Label'].values\n",
    "    \n",
    "    # For binary classification: Convert labels to binary (Benign: 0, Attack: 1)\n",
    "    y_binary = np.where(y == 'BENIGN', 0, 1)\n",
    "    \n",
    "    # For multiclass classification: Encode labels\n",
    "    unique_labels = np.unique(y)\n",
    "    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y_multiclass = np.array([label_map[label] for label in y])\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y_binary, y_multiclass, unique_labels\n",
    "\n",
    "# 2. Graph Construction\n",
    "def build_graph(X, threshold=0.9):\n",
    "    num_nodes = X.shape[0]\n",
    "    edge_index = []\n",
    "    \n",
    "    # Compute cosine similarity for edges\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            similarity = 1 - cosine(X[i], X[j])\n",
    "            if similarity > threshold:\n",
    "                edge_index.append([i, j])\n",
    "                edge_index.append([j, i])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(X, dtype=torch.float)\n",
    "    \n",
    "    return edge_index, x\n",
    "\n",
    "# 3. GAT Model Definition\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=8):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=0.6)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 4. Training and Evaluation\n",
    "def train_and_evaluate(model, data, y, num_epochs, is_binary=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_mask = data.train_mask\n",
    "    val_mask = data.val_mask\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[train_mask], y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data.x, data.edge_index)\n",
    "            val_loss = criterion(val_out[val_mask], y[val_mask])\n",
    "            pred = out.argmax(dim=1)\n",
    "            val_pred = pred[val_mask]\n",
    "            val_true = y[val_mask]\n",
    "            \n",
    "            acc = accuracy_score(val_true, val_pred)\n",
    "            prec = precision_score(val_true, val_pred, average='weighted', zero_division=0)\n",
    "            rec = recall_score(val_true, val_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(val_true, val_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            if is_binary:\n",
    "                probs = F.softmax(out, dim=1)[:, 1]\n",
    "                auc = roc_auc_score(val_true, probs[val_mask].numpy())\n",
    "            else:\n",
    "                auc = 0  # AUC for multiclass requires one-vs-rest, simplified here\n",
    "            \n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['val_loss'].append(val_loss.item())\n",
    "            history['val_acc'].append(acc)\n",
    "            history['val_prec'].append(prec)\n",
    "            history['val_rec'].append(rec)\n",
    "            history['val_f1'].append(f1)\n",
    "            history['val_auc'].append(auc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={loss.item():.4f}, Val Loss={val_loss.item():.4f}, Val Acc={acc:.4f}')\n",
    "    \n",
    "    return history, pred, F.softmax(out, dim=1)\n",
    "\n",
    "# 5. Plotting\n",
    "def plot_metrics(history, title_prefix):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title(f'{title_prefix} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{title_prefix} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. ROC Curve\n",
    "def plot_roc_curve(y_true, y_probs, is_binary, unique_labels):\n",
    "    plt.figure()\n",
    "    if is_binary:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "        auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "    else:\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            fpr, tpr, _ = roc_curve(y_true == i, y_probs[:, i])\n",
    "            auc = roc_auc_score(y_true == i, y_probs[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'{label} (AUC = {auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    file_path = '../dataset/CICIDS2017/CICIDS-2017.csv'  # Update with actual path\n",
    "    X, y_binary, y_multiclass, unique_labels = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # Build graph\n",
    "    edge_index, x = build_graph(X)\n",
    "    \n",
    "    # Create train/test masks\n",
    "    train_idx, val_idx = train_test_split(range(X.shape[0]), test_size=0.2, random_state=42)\n",
    "    train_mask = torch.zeros(X.shape[0], dtype=torch.bool)\n",
    "    val_mask = torch.zeros(X.shape[0], dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    \n",
    "    # Binary Classification\n",
    "    data = Data(x=x, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask)\n",
    "    model_binary = GATModel(in_channels=X.shape[1], hidden_channels=16, out_channels=2)\n",
    "    history_binary, pred_binary, probs_binary = train_and_evaluate(model_binary, data, torch.tensor(y_binary, dtype=torch.long), num_epochs=50, is_binary=True)\n",
    "    \n",
    "    # Evaluate Binary Classification\n",
    "    print(\"\\nBinary Classification Metrics:\")\n",
    "    print(f\"Accuracy: {history_binary['val_acc'][-1]:.4f}\")\n",
    "    print(f\"Precision: {history_binary['val_prec'][-1]:.4f}\")\n",
    "    print(f\"Recall: {history_binary['val_rec'][-1]:.4f}\")\n",
    "    print(f\"F1 Score: {history_binary['val_f1'][-1]:.4f}\")\n",
    "    print(f\"AUC: {history_binary['val_auc'][-1]:.4f}\")\n",
    "    \n",
    "    plot_metrics(history_binary, \"Binary Classification\")\n",
    "    plot_roc_curve(y_binary[val_mask], probs_binary[val_mask].numpy(), is_binary=True, unique_labels=['Benign', 'Malicious'])\n",
    "    \n",
    "    # Multiclass Classification\n",
    "    data = Data(x=x, edge_index=edge_index, train_mask=train_mask, val_mask=val_mask)\n",
    "    model_multiclass = GATModel(in_channels=X.shape[1], hidden_channels=16, out_channels=len(unique_labels))\n",
    "    history_multiclass, pred_multiclass, probs_multiclass = train_and_evaluate(model_multiclass, data, torch.tensor(y_multiclass, dtype=torch.long), num_epochs=50, is_binary=False)\n",
    "    \n",
    "    # Evaluate Multiclass Classification\n",
    "    print(\"\\nMulticlass Classification Metrics:\")\n",
    "    print(f\"Accuracy: {history_multiclass['val_acc'][-1]:.4f}\")\n",
    "    print(f\"Precision: {history_multiclass['val_prec'][-1]:.4f}\")\n",
    "    print(f\"Recall: {history_multiclass['val_rec'][-1]:.4f}\")\n",
    "    print(f\"F1 Score: {history_multiclass['val_f1'][-1]:.4f}\")\n",
    "    \n",
    "    plot_metrics(history_multiclass, \"Multiclass Classification\")\n",
    "    plot_roc_curve(y_multiclass[val_mask], probs_multiclass[val_mask].numpy(), is_binary=False, unique_labels=unique_labels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83c13b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading subset of data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/Monday-WorkingHours.pcap_ISCX.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading subset of data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m data \u001b[38;5;241m=\u001b[39m load_subset(sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m)  \u001b[38;5;66;03m# Adjust sample_size based on your memory\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m## Step 2: Feature Engineering and Splitting\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mload_subset\u001b[1;34m(sample_size, random_state)\u001b[0m\n\u001b[0;32m     24\u001b[0m data_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/Monday-WorkingHours.pcap_ISCX.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/Tuesday-WorkingHours.pcap_ISCX.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     33\u001b[0m ]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load first file to get columns\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_files[\u001b[38;5;241m0\u001b[39m], nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Load subset of data\u001b[39;00m\n\u001b[0;32m     39\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/Monday-WorkingHours.pcap_ISCX.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix, \n",
    "                            roc_curve, precision_recall_curve)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "## Step 1: Data Loading and Preprocessing\n",
    "print(\"Loading and preprocessing data...\")\n",
    "\n",
    "# Load all dataset files\n",
    "data_files = [\n",
    "    '../dataset/data/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    '../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "# Load and concatenate all datasets\n",
    "data_list = []\n",
    "for file in data_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        data_list.append(df)\n",
    "        print(f\"Loaded {file} with shape {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "data = pd.concat(data_list, axis=0)\n",
    "print(f\"Final dataset shape: {data.shape}\")\n",
    "\n",
    "# Clean up memory\n",
    "del data_list\n",
    "del df\n",
    "\n",
    "# Strip whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Handle missing and infinite values\n",
    "print(\"\\nHandling missing and infinite values...\")\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values with median\n",
    "for col in data.columns:\n",
    "    if data[col].dtype in [np.float64, np.int64]:\n",
    "        med = data[col].median()\n",
    "        data[col].fillna(med, inplace=True)\n",
    "\n",
    "# Check for remaining missing values\n",
    "missing_values = data.isna().sum()\n",
    "print(f\"Missing values after imputation:\\n{missing_values[missing_values > 0]}\")\n",
    "\n",
    "# Label encoding for categorical features and target\n",
    "print(\"\\nEncoding labels...\")\n",
    "label_encoder = LabelEncoder()\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Binary classification (normal vs attack)\n",
    "data['Label'] = data['Label'].apply(lambda x: 0 if x == 0 else 1)  # 0: normal, 1: attack\n",
    "\n",
    "# Show class distribution\n",
    "class_dist = data['Label'].value_counts(normalize=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_dist)\n",
    "\n",
    "# Feature selection - remove constant and duplicate features\n",
    "print(\"\\nRemoving constant and duplicate features...\")\n",
    "constant_features = [col for col in data.columns if data[col].nunique() == 1]\n",
    "duplicate_features = []\n",
    "for i, col1 in enumerate(data.columns):\n",
    "    for col2 in data.columns[i+1:]:\n",
    "        if data[col1].equals(data[col2]):\n",
    "            duplicate_features.append(col2)\n",
    "\n",
    "features_to_drop = list(set(constant_features + duplicate_features))\n",
    "data.drop(features_to_drop, axis=1, inplace=True)\n",
    "print(f\"Dropped {len(features_to_drop)} features\")\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Standardize features\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train/test sets (stratified to maintain class distribution)\n",
    "# After train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to numpy arrays first if they're pandas Series\n",
    "if hasattr(y_train, 'values'):\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "# Then convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "## Step 2: Graph Construction\n",
    "print(\"\\nConstructing graph...\")\n",
    "\n",
    "def create_graph_data(X, y, k=5):\n",
    "    \"\"\"Create graph data from tabular data using k-NN\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    distances = torch.cdist(X_tensor, X_tensor)\n",
    "    \n",
    "    # Create k-NN adjacency matrix\n",
    "    _, indices = torch.topk(distances, k=k, largest=False)\n",
    "    \n",
    "    # Create edge_index\n",
    "    edge_index = []\n",
    "    for i in range(len(indices)):\n",
    "        for j in indices[i]:\n",
    "            edge_index.append([i, j])\n",
    "    \n",
    "    edge_index = torch.LongTensor(edge_index).t().contiguous()\n",
    "    \n",
    "    # Create edge attributes (optional)\n",
    "    edge_attr = distances[edge_index[0], edge_index[1]].unsqueeze(1)\n",
    "    \n",
    "    # Create graph data\n",
    "    graph_data = Data(\n",
    "        x=X_tensor,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=torch.LongTensor(y)\n",
    "    )\n",
    "    \n",
    "    return graph_data\n",
    "\n",
    "# Create graph data for training and testing\n",
    "train_graph = create_graph_data(X_train, y_train, k=5)\n",
    "test_graph = create_graph_data(X_test, y_test, k=5)\n",
    "\n",
    "# Move graphs to device\n",
    "train_graph = train_graph.to(device)\n",
    "test_graph = test_graph.to(device)\n",
    "\n",
    "## Step 3: Attention-based GNN Model with Class Weighting\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64, heads=4, dropout=0.2):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv3 = GATConv(hidden_dim * heads, hidden_dim, heads=1, dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, batch=None)\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "model = GATModel(num_features, num_classes, hidden_dim=64, heads=4).to(device)\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float32)\n",
    "class_weights = class_weights.to(device)\n",
    "criterion = nn.NLLLoss(weight=class_weights)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "## Step 4: Training and Evaluation\n",
    "def train(model, graph):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph)\n",
    "    loss = criterion(out, graph.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred == graph.y).sum().item()\n",
    "        acc = correct / len(graph.y)\n",
    "        loss = criterion(out, graph.y).item()\n",
    "    return acc, loss\n",
    "\n",
    "def test(model, graph):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph)\n",
    "        pred = out.argmax(dim=1)\n",
    "        prob = torch.exp(out)[:, 1]  # Probability of class 1 (attack)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(graph.y.cpu(), pred.cpu())\n",
    "        precision = precision_score(graph.y.cpu(), pred.cpu())\n",
    "        recall = recall_score(graph.y.cpu(), pred.cpu())\n",
    "        f1 = f1_score(graph.y.cpu(), pred.cpu())\n",
    "        auc = roc_auc_score(graph.y.cpu(), prob.cpu())\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(graph.y.cpu(), pred.cpu())\n",
    "        \n",
    "        # ROC curve data\n",
    "        fpr, tpr, _ = roc_curve(graph.y.cpu(), prob.cpu())\n",
    "        \n",
    "        # Precision-recall curve\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(graph.y.cpu(), prob.cpu())\n",
    "        \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'precision_curve': precision_curve,\n",
    "        'recall_curve': recall_curve\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining model...\")\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "best_val_acc = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    loss = train(model, train_graph)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    train_acc, _ = evaluate(model, train_graph)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    val_acc, val_loss = evaluate(model, test_graph)\n",
    "    val_accs.append(val_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = model.state_dict().copy()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Step 5: Final Evaluation\n",
    "print(\"\\nFinal evaluation on test set...\")\n",
    "test_results = test(model, test_graph)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {test_results['precision']:.4f}\")\n",
    "print(f\"Test Recall: {test_results['recall']:.4f}\")\n",
    "print(f\"Test F1 Score: {test_results['f1']:.4f}\")\n",
    "print(f\"Test AUC: {test_results['auc']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(test_results['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Normal', 'Attack'], \n",
    "            yticklabels=['Normal', 'Attack'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(test_results['fpr'], test_results['tpr'], label=f'ROC Curve (AUC = {test_results[\"auc\"]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(test_results['recall_curve'], test_results['precision_curve'], label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6a6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading subset of data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_15004\\57752974.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(med, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (50000, 69)\n",
      "Creating mini-batch graphs...\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (1000).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 184\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[1;32m--> 184\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_mini_batch(model, train_graphs, optimizer)\n\u001b[0;32m    185\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m evaluate_mini_batch(model, train_graphs)\n\u001b[0;32m    186\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m evaluate_mini_batch(model, test_graphs)\n",
      "Cell \u001b[1;32mIn[4], line 164\u001b[0m, in \u001b[0;36mtrain_mini_batch\u001b[1;34m(model, graphs, optimizer)\u001b[0m\n\u001b[0;32m    162\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    163\u001b[0m out \u001b[38;5;241m=\u001b[39m model(graph)\n\u001b[1;32m--> 164\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, graph\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m    165\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    166\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:251\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnll_loss(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    253\u001b[0m         target,\n\u001b[0;32m    254\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    255\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m    256\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m    257\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3158\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   3156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3157\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mnll_loss_nd(\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index\n\u001b[0;32m   3160\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (1000)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "## Step 1: Load and Preprocess a Subset of Data\n",
    "def load_subset(sample_size=50000, random_state=42):\n",
    "    \"\"\"Load a subset of the CICIDS2017 data\"\"\"\n",
    "    data_files = [\n",
    "        '../dataset/data/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "        '../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "        '../dataset/data/Wednesday-workingHours.pcap_ISCX.csv',\n",
    "        '../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "        '../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "        '../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "        '../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "        '../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "    ]\n",
    "    \n",
    "    # Load first file to get columns\n",
    "    sample_df = pd.read_csv(data_files[0], nrows=1)\n",
    "    \n",
    "    # Load subset of data\n",
    "    chunks = []\n",
    "    for file in data_files:\n",
    "        for chunk in pd.read_csv(file, chunksize=sample_size//len(data_files)):\n",
    "            chunks.append(chunk)\n",
    "            if len(chunks) >= len(data_files):\n",
    "                break\n",
    "    \n",
    "    data = pd.concat(chunks, axis=0)\n",
    "    data = data.sample(min(sample_size, len(data)), random_state=random_state)\n",
    "    \n",
    "    # Preprocessing\n",
    "    data.columns = data.columns.str.strip()\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype in [np.float64, np.int64]:\n",
    "            med = data[col].median()\n",
    "            data[col].fillna(med, inplace=True)\n",
    "    \n",
    "    # Binary classification\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "    data['Label'] = data['Label'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    # Remove constant features\n",
    "    nunique = data.nunique()\n",
    "    constant_cols = nunique[nunique == 1].index\n",
    "    data.drop(constant_cols, axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Loading subset of data...\")\n",
    "data = load_subset(sample_size=50000)  # Adjust sample_size based on your memory\n",
    "print(f\"Loaded data shape: {data.shape}\")\n",
    "\n",
    "## Step 2: Feature Engineering and Splitting\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "## Step 3: Efficient Graph Construction\n",
    "def create_mini_batch_graphs(X, y, batch_size=1000, k=5):\n",
    "    \"\"\"Create multiple smaller graphs instead of one large graph\"\"\"\n",
    "    graphs = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X_batch)\n",
    "        distances = torch.cdist(X_tensor, X_tensor)\n",
    "        \n",
    "        # Create k-NN edges\n",
    "        _, indices = torch.topk(distances, k=k, largest=False)\n",
    "        edge_index = []\n",
    "        for i in range(len(indices)):\n",
    "            for j in indices[i]:\n",
    "                edge_index.append([i, j])\n",
    "        \n",
    "        edge_index = torch.LongTensor(edge_index).t().contiguous()\n",
    "        \n",
    "        graphs.append(Data(\n",
    "            x=X_tensor,\n",
    "            edge_index=edge_index,\n",
    "            y=torch.LongTensor(y_batch.values if hasattr(y_batch, 'values') else y_batch)\n",
    "        ))\n",
    "    return graphs\n",
    "\n",
    "print(\"Creating mini-batch graphs...\")\n",
    "train_graphs = create_mini_batch_graphs(X_train, y_train, batch_size=1000)\n",
    "test_graphs = create_mini_batch_graphs(X_test, y_test, batch_size=1000)\n",
    "\n",
    "# Move to device\n",
    "train_graphs = [g.to(device) for g in train_graphs]\n",
    "test_graphs = [g.to(device) for g in test_graphs]\n",
    "\n",
    "## Step 4: Attention-based GNN Model\n",
    "class MiniBatchGAT(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64, heads=4, dropout=0.2):\n",
    "        super(MiniBatchGAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim*heads, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv3 = GATConv(hidden_dim*heads, hidden_dim, heads=1, dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        \n",
    "        x = global_mean_pool(x, batch=None)\n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model\n",
    "num_features = X_train.shape[1]\n",
    "model = MiniBatchGAT(num_features, 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Class weights for imbalanced data\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = torch.FloatTensor([1./class_counts[0], 1./class_counts[1]]).to(device)\n",
    "criterion = nn.NLLLoss(weight=class_weights)\n",
    "\n",
    "## Step 5: Training and Evaluation\n",
    "def train_mini_batch(model, graphs, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for graph in graphs:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph)\n",
    "        loss = criterion(out, graph.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(graphs)\n",
    "\n",
    "def evaluate_mini_batch(model, graphs):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for graph in graphs:\n",
    "            out = model(graph)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == graph.y).sum().item()\n",
    "            total += len(graph.y)\n",
    "    return correct / total\n",
    "\n",
    "print(\"Training model...\")\n",
    "for epoch in range(1, 101):\n",
    "    loss = train_mini_batch(model, train_graphs, optimizer)\n",
    "    train_acc = evaluate_mini_batch(model, train_graphs)\n",
    "    test_acc = evaluate_mini_batch(model, test_graphs)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "## Step 6: Final Evaluation\n",
    "def evaluate_metrics(model, graphs):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for graph in graphs:\n",
    "            out = model(graph)\n",
    "            pred = out.argmax(dim=1)\n",
    "            prob = torch.exp(out)[:, 1]\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_probs.extend(prob.cpu().numpy())\n",
    "            all_labels.extend(graph.y.cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds),\n",
    "        'auc': roc_auc_score(all_labels, all_probs),\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds)\n",
    "    }\n",
    "\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "results = evaluate_metrics(model, test_graphs)\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall: {results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {results['f1']:.4f}\")\n",
    "print(f\"AUC: {results['auc']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(results['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb377d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c3c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (replace with your actual paths)\n",
    "data1 = pd.read_csv('../dataset/data/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('../dataset/data/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv('../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9789be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "Data1 -> 529918 rows, 79 columns\n",
      "Data2 -> 445909 rows, 79 columns\n",
      "Data3 -> 692703 rows, 79 columns\n",
      "Data4 -> 170366 rows, 79 columns\n",
      "Data5 -> 288602 rows, 79 columns\n",
      "Data6 -> 191033 rows, 79 columns\n",
      "Data7 -> 286467 rows, 79 columns\n",
      "Data8 -> 225745 rows, 79 columns\n",
      "New dimension:\n",
      "Number of rows: 2830743\n",
      "Number of columns: 79\n",
      "Total cells: 223628697\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Data Loading\n",
    "data1 = pd.read_csv('../dataset/data/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('../dataset/data/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv('../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "data_list = [data1, data2, data3, data4, data5, data6, data7, data8]\n",
    "\n",
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start=1):\n",
    "    rows, cols = data.shape\n",
    "    print(f'Data{i} -> {rows} rows, {cols} columns')\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat(data_list)\n",
    "rows, cols = data.shape\n",
    "print('New dimension:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea128d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "for d in data_list:\n",
    "    del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc2e462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49188</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49188</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49188</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49188</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49486</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
       "0              49188               4                   2   \n",
       "1              49188               1                   2   \n",
       "2              49188               1                   2   \n",
       "3              49188               1                   2   \n",
       "4              49486               3                   2   \n",
       "\n",
       "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
       "0                        0                           12   \n",
       "1                        0                           12   \n",
       "2                        0                           12   \n",
       "3                        0                           12   \n",
       "4                        0                           12   \n",
       "\n",
       "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
       "0                             0                       6   \n",
       "1                             0                       6   \n",
       "2                             0                       6   \n",
       "3                             0                       6   \n",
       "4                             0                       6   \n",
       "\n",
       "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
       "0                       6                      6.0                     0.0   \n",
       "1                       6                      6.0                     0.0   \n",
       "2                       6                      6.0                     0.0   \n",
       "3                       6                      6.0                     0.0   \n",
       "4                       6                      6.0                     0.0   \n",
       "\n",
       "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
       "0  ...                     20          0.0          0.0            0   \n",
       "1  ...                     20          0.0          0.0            0   \n",
       "2  ...                     20          0.0          0.0            0   \n",
       "3  ...                     20          0.0          0.0            0   \n",
       "4  ...                     20          0.0          0.0            0   \n",
       "\n",
       "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
       "0            0        0.0        0.0          0          0  BENIGN  \n",
       "1            0        0.0        0.0          0          0  BENIGN  \n",
       "2            0        0.0        0.0          0          0  BENIGN  \n",
       "3            0        0.0        0.0          0          0  BENIGN  \n",
       "4            0        0.0        0.0          0          0  BENIGN  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3f8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 308381\n"
     ]
    }
   ],
   "source": [
    "dups = data[data.duplicated()]\n",
    "print(f'Number of duplicates: {len(dups)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da14ed4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2522362, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean column names\n",
    "data.columns = data.columns.str.strip()\n",
    "data.drop_duplicates(inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f80332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Bytes/s    353\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_val = data.isna().sum()\n",
    "print(missing_val.loc[missing_val > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3027c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow Bytes/s      1211\n",
      "Flow Packets/s    1564\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for infinity values\n",
    "numeric_cols = data.select_dtypes(include = np.number).columns\n",
    "inf_count = np.isinf(data[numeric_cols]).sum()\n",
    "print(inf_count[inf_count > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2e0884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial missing values: 353\n",
      "Missing values after processing infinite values: 3128\n"
     ]
    }
   ],
   "source": [
    "# Replacing any infinite values (positive or negative) with NaN (not a number)\n",
    "print(f'Initial missing values: {data.isna().sum().sum()}')\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "\n",
    "print(f'Missing values after processing infinite values: {data.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d75872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_2316\\1003562915.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing and infinite values\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "for col in data.select_dtypes(include=[np.float64, np.float32]).columns:\n",
    "    data[col].fillna(data[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be41d70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'FTP-Patator', 'SSH-Patator', 'DoS slowloris',\n",
       "       'DoS Slowhttptest', 'DoS Hulk', 'DoS GoldenEye', 'Heartbleed',\n",
       "       'Web Attack � Brute Force', 'Web Attack � XSS',\n",
       "       'Web Attack � Sql Injection', 'Infiltration', 'Bot', 'PortScan',\n",
       "       'DDoS'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c490bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "BENIGN                        2096484\n",
       "DoS Hulk                       172849\n",
       "DDoS                           128016\n",
       "PortScan                        90819\n",
       "DoS GoldenEye                   10286\n",
       "FTP-Patator                      5933\n",
       "DoS slowloris                    5385\n",
       "DoS Slowhttptest                 5228\n",
       "SSH-Patator                      3219\n",
       "Bot                              1953\n",
       "Web Attack � Brute Force         1470\n",
       "Web Attack � XSS                  652\n",
       "Infiltration                       36\n",
       "Web Attack � Sql Injection         21\n",
       "Heartbleed                         11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types of attacks & normal instances (BENIGN)\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef884e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map attack types\n",
    "attack_map = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'DDoS': 'DDoS',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'PortScan': 'Port Scan',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Botnet',\n",
    "    'Web Attack � Brute Force': 'Web Attack',\n",
    "    'Web Attack � XSS': 'Web Attack',\n",
    "    'Web Attack � Sql Injection': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Heartbleed': 'Heartbleed'\n",
    "}\n",
    "data['Attack Type'] = data['Label'].map(attack_map)\n",
    "data.drop('Label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "044f9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode attack types to integers\n",
    "unique_labels = data['Attack Type'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "data['Attack Type'] = data['Attack Type'].map(label_to_idx)\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Select numerical features (exclude non-numerical columns)\n",
    "feature_cols = [col for col in data.columns if col != 'Attack Type' and data[col].dtype in [np.float64, np.float32, np.int64]]\n",
    "X = data[feature_cols].values\n",
    "y = data['Attack Type'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf6515a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 46.3 TiB for an array with shape (2522362, 2522362) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: Graph Construction\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Create adjacency matrix based on feature similarity (cosine similarity)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m adj_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_nodes, num_nodes))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, num_nodes):\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 46.3 TiB for an array with shape (2522362, 2522362) and data type float64"
     ]
    }
   ],
   "source": [
    "# Step 3: Graph Construction\n",
    "# Create adjacency matrix based on feature similarity (cosine similarity)\n",
    "num_nodes = X.shape[0]\n",
    "adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "for i in range(num_nodes):\n",
    "    for j in range(i + 1, num_nodes):\n",
    "        dot_product = np.sum(X[i] * X[j])\n",
    "        norm_i = np.sqrt(np.sum(X[i] ** 2))\n",
    "        norm_j = np.sqrt(np.sum(X[j] ** 2))\n",
    "        similarity = dot_product / (norm_i * norm_j + 1e-8)\n",
    "        if similarity > 0.8:  # Threshold for edge creation\n",
    "            adj_matrix[i, j] = 1\n",
    "            adj_matrix[j, i] = 1\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "adj_tensor = torch.FloatTensor(adj_matrix)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "train_idx, test_idx = train_test_split(range(num_nodes), test_size=0.2, stratify=y, random_state=42)\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d8db6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: \n",
      "Data1 -> 529918 rows, 79 columns\n",
      "Data2 -> 445909 rows, 79 columns\n",
      "Data3 -> 692703 rows, 79 columns\n",
      "Data4 -> 170366 rows, 79 columns\n",
      "Data5 -> 288602 rows, 79 columns\n",
      "Data6 -> 191033 rows, 79 columns\n",
      "Data7 -> 286467 rows, 79 columns\n",
      "Data8 -> 225745 rows, 79 columns\n",
      "New dimension:\n",
      "Number of rows: 2830743\n",
      "Number of columns: 79\n",
      "Total cells: 223628697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_15720\\2967762536.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].median(), inplace=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Map attack types\u001b[39;00m\n\u001b[0;32m     45\u001b[0m attack_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBENIGN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBENIGN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDoS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDoS\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeartbleed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeartbleed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m }\n\u001b[1;32m---> 62\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttack Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(attack_map)\n\u001b[0;32m     63\u001b[0m data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Encode attack types to integers\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Label'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Data Loading\n",
    "data1 = pd.read_csv('../dataset/data/Monday-WorkingHours.pcap_ISCX.csv')\n",
    "data2 = pd.read_csv('../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "data3 = pd.read_csv('../dataset/data/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "data4 = pd.read_csv('../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "data5 = pd.read_csv('../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "data6 = pd.read_csv('../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "data7 = pd.read_csv('../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "data8 = pd.read_csv('../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "data_list = [data1, data2, data3, data4, data5, data6, data7, data8]\n",
    "\n",
    "print('Data dimensions: ')\n",
    "for i, data in enumerate(data_list, start=1):\n",
    "    rows, cols = data.shape\n",
    "    print(f'Data{i} -> {rows} rows, {cols} columns')\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat(data_list)\n",
    "rows, cols = data.shape\n",
    "print('New dimension:')\n",
    "print(f'Number of rows: {rows}')\n",
    "print(f'Number of columns: {cols}')\n",
    "print(f'Total cells: {rows * cols}')\n",
    "\n",
    "# Free memory\n",
    "for d in data_list:\n",
    "    del d\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "# Handle missing and infinite values\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "for col in data.select_dtypes(include=[np.float64, np.float32]).columns:\n",
    "    data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Map attack types\n",
    "attack_map = {\n",
    "    'BENIGN': 'BENIGN',\n",
    "    'DDoS': 'DDoS',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'PortScan': 'Port Scan',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'Bot': 'Botnet',\n",
    "    'Web Attack � Brute Force': 'Web Attack',\n",
    "    'Web Attack � XSS': 'Web Attack',\n",
    "    'Web Attack � Sql Injection': 'Web Attack',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Heartbleed': 'Heartbleed'\n",
    "}\n",
    "data['Attack Type'] = data['Label'].map(attack_map)\n",
    "data.drop('Label', axis=1, inplace=True)\n",
    "\n",
    "# Encode attack types to integers\n",
    "unique_labels = data['Attack Type'].unique()\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "data['Attack Type'] = data['Attack Type'].map(label_to_idx)\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# Select numerical features (exclude non-numerical columns)\n",
    "feature_cols = [col for col in data.columns if col != 'Attack Type' and data[col].dtype in [np.float64, np.float32, np.int64]]\n",
    "X = data[feature_cols].values\n",
    "y = data['Attack Type'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Graph Construction\n",
    "# Create adjacency matrix based on feature similarity (cosine similarity)\n",
    "num_nodes = X.shape[0]\n",
    "adj_matrix = np.zeros((num_nodes, num_nodes))\n",
    "for i in range(num_nodes):\n",
    "    for j in range(i + 1, num_nodes):\n",
    "        dot_product = np.sum(X[i] * X[j])\n",
    "        norm_i = np.sqrt(np.sum(X[i] ** 2))\n",
    "        norm_j = np.sqrt(np.sum(X[j] ** 2))\n",
    "        similarity = dot_product / (norm_i * norm_j + 1e-8)\n",
    "        if similarity > 0.8:  # Threshold for edge creation\n",
    "            adj_matrix[i, j] = 1\n",
    "            adj_matrix[j, i] = 1\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "adj_tensor = torch.FloatTensor(adj_matrix)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "train_idx, test_idx = train_test_split(range(num_nodes), test_size=0.2, stratify=y, random_state=42)\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Step 5: GNN with Attention Mechanism\n",
    "class GNNAttention(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNAttention, self).__init__()\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
    "        self.W_att = torch.nn.Parameter(torch.randn(hidden_dim, 1))\n",
    "        self.W_out = torch.nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        # First GNN layer\n",
    "        h1 = torch.matmul(X, self.W1)\n",
    "        h1 = self.relu(torch.matmul(adj, h1))\n",
    "\n",
    "        # Attention mechanism\n",
    "        att_scores = torch.matmul(h1, self.W_att)\n",
    "        att_weights = torch.nn.functional.softmax(att_scores, dim=0)\n",
    "        h1_att = h1 * att_weights\n",
    "\n",
    "        # Second GNN layer\n",
    "        h2 = torch.matmul(h1_att, self.W2)\n",
    "        h2 = self.relu(torch.matmul(adj, h2))\n",
    "\n",
    "        # Output layer\n",
    "        out = torch.matmul(h2, self.W_out)\n",
    "        return out\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 64\n",
    "model = GNNAttention(input_dim, hidden_dim, num_classes)\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters(), 'lr': 0.01}])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 6: Training\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X_tensor, adj_tensor)\n",
    "    loss = criterion(out[train_mask], y_tensor[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Training accuracy\n",
    "    _, pred = torch.max(out[train_mask], 1)\n",
    "    train_acc = accuracy_score(y_tensor[train_mask].numpy(), pred.numpy())\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Validation (using test set as validation here)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(X_tensor, adj_tensor)\n",
    "        val_loss = criterion(out[test_mask], y_tensor[test_mask])\n",
    "        val_losses.append(val_loss.item())\n",
    "        _, pred = torch.max(out[test_mask], 1)\n",
    "        val_acc = accuracy_score(y_tensor[test_mask].numpy(), pred.numpy())\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Step 7: Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(X_tensor, adj_tensor)\n",
    "    _, pred = torch.max(out[test_mask], 1)\n",
    "    y_true = y_tensor[test_mask].numpy()\n",
    "    y_pred = pred.numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f'Final Test Metrics:')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6127ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Processing ../dataset/data/Monday-WorkingHours.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Wednesday-workingHours.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (283074, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\1894817497.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data files\n",
    "data_files = [\n",
    "    '../dataset/data/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    '../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "# Process data in chunks\n",
    "chunk_size = 100000  # Adjust based on memory\n",
    "data_chunks = []\n",
    "\n",
    "for file in data_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    try:\n",
    "        # Read in chunks\n",
    "        for chunk in pd.read_csv(file, chunksize=chunk_size, low_memory=False):\n",
    "            # Strip whitespace from columns\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            # Replace inf/nan\n",
    "            chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            # Fill numeric missing values with median\n",
    "            for col in chunk.select_dtypes(include=[np.float64, np.int64]).columns:\n",
    "                chunk[col].fillna(chunk[col].median(), inplace=True)\n",
    "            # Map attack types\n",
    "            attack_map = {\n",
    "                'BENIGN': 'BENIGN', 'DDoS': 'DDoS', 'DoS Hulk': 'DoS', 'DoS GoldenEye': 'DoS',\n",
    "                'DoS slowloris': 'DoS', 'DoS Slowhttptest': 'DoS', 'PortScan': 'Port Scan',\n",
    "                'FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force', 'Bot': 'Botnet',\n",
    "                'Web Attack � Brute Force': 'Web Attack', 'Web Attack � XSS': 'Web Attack',\n",
    "                'Web Attack � Sql Injection': 'Web Attack', 'Infiltration': 'Infiltration',\n",
    "                'Heartbleed': 'Heartbleed'\n",
    "            }\n",
    "            chunk['Attack Type'] = chunk['Label'].map(attack_map)\n",
    "            chunk.drop('Label', axis=1, inplace=True)\n",
    "            # Downsample to reduce memory (e.g., take 10% of each chunk)\n",
    "            chunk = chunk.sample(frac=0.1, random_state=42)\n",
    "            data_chunks.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Concatenate chunks\n",
    "data = pd.concat(data_chunks, axis=0)\n",
    "print(f\"Final dataset shape: {data.shape}\")\n",
    "\n",
    "# Free memory\n",
    "del data_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9808cfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: torch.Size([283074, 19])\n",
      "Label tensor shape: torch.Size([283074])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Select features (example: choose a subset to reduce memory)\n",
    "features = [\n",
    "    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "    'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',\n",
    "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "\n",
    "# Ensure features exist in data\n",
    "features = [f for f in features if f in data.columns]\n",
    "X = data[features]\n",
    "y = data['Attack Type']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(device)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_tensor.shape}\")\n",
    "print(f\"Label tensor shape: {y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4709ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph created with 283074 nodes and 1844152 edges\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch_geometric.utils import to_undirected, dense_to_sparse\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Create adjacency matrix using k-nearest neighbors (k=5 for sparsity)\n",
    "n_samples = X_scaled.shape[0]\n",
    "k = 5  # Number of neighbors\n",
    "adj_matrix = kneighbors_graph(X_scaled, n_neighbors=k, mode='connectivity', include_self=False)\n",
    "\n",
    "# Convert sparse matrix to COO format and extract edge indices\n",
    "adj_matrix = adj_matrix.tocoo()  # Convert to COO format for easy edge extraction\n",
    "row = torch.tensor(adj_matrix.row, dtype=torch.long)\n",
    "col = torch.tensor(adj_matrix.col, dtype=torch.long)\n",
    "edge_index = torch.stack([row, col], dim=0)  # Shape: [2, num_edges]\n",
    "\n",
    "# Make graph undirected\n",
    "edge_index = to_undirected(edge_index)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "graph_data = Data(x=X_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(f\"Graph created with {graph_data.num_nodes} nodes and {graph_data.num_edges} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7106530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn  # Ensure this import is present\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.utils import to_undirected\n",
    "import scipy.sparse as sp\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "# Step 4: Define GAT Model\n",
    "class GATNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):\n",
    "        super(GATNetwork, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.2)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long, device=x.device))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983fe5a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Found indices in 'edge_index' that are larger than 31 (got 283073). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 32) in your node feature matrix and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:271\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m Data(\n\u001b[0;32m     18\u001b[0m     x\u001b[38;5;241m=\u001b[39mgraph_data\u001b[38;5;241m.\u001b[39mx[batch_mask],\n\u001b[0;32m     19\u001b[0m     edge_index\u001b[38;5;241m=\u001b[39mgraph_data\u001b[38;5;241m.\u001b[39medge_index,\n\u001b[0;32m     20\u001b[0m     y\u001b[38;5;241m=\u001b[39mgraph_data\u001b[38;5;241m.\u001b[39my[batch_mask]\n\u001b[0;32m     21\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 23\u001b[0m out \u001b[38;5;241m=\u001b[39m model(batch_data)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, batch_data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m, in \u001b[0;36mGATNetwork.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     27\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m---> 28\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index))\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index))\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:362\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    357\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe usage of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_self_loops\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimultaneously is currently not yet supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseTensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m form\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# edge_updater_type: (alpha: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index, alpha\u001b[38;5;241m=\u001b[39malpha, edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[0;32m    363\u001b[0m                           size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, alpha\u001b[38;5;241m=\u001b[39malpha, size\u001b[38;5;241m=\u001b[39msize)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_8n4kh16x.py:145\u001b[0m, in \u001b[0;36medge_updater\u001b[1;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21medge_updater\u001b[39m(\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    137\u001b[0m     edge_index: Union[Tensor, SparseTensor],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     size: Size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    143\u001b[0m     mutable_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(edge_index, size)\n\u001b[1;32m--> 145\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_collect(\n\u001b[0;32m    146\u001b[0m         edge_index,\n\u001b[0;32m    147\u001b[0m         alpha,\n\u001b[0;32m    148\u001b[0m         edge_attr,\n\u001b[0;32m    149\u001b[0m         mutable_size,\n\u001b[0;32m    150\u001b[0m     )\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# Begin Edge Update Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\torch_geometric.nn.conv.gat_conv_GATConv_edge_updater_8n4kh16x.py:91\u001b[0m, in \u001b[0;36medge_collect\u001b[1;34m(self, edge_index, alpha, edge_attr, size)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_alpha_0, Tensor):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, \u001b[38;5;241m0\u001b[39m, _alpha_0)\n\u001b[1;32m---> 91\u001b[0m     alpha_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select(_alpha_0, edge_index_j)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     alpha_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:267\u001b[0m, in \u001b[0;36mMessagePassing._index_select\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim, index)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_select_safe(src, index)\n",
      "File \u001b[1;32mc:\\Users\\gech\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:282\u001b[0m, in \u001b[0;36mMessagePassing._index_select_safe\u001b[1;34m(self, src, index)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)):\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m that are larger \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mIndexError\u001b[0m: Found indices in 'edge_index' that are larger than 31 (got 283073). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 32) in your node feature matrix and try again."
     ]
    }
   ],
   "source": [
    "# Step 5: Training\n",
    "num_features = X_tensor.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = GATNetwork(in_channels=num_features, hidden_channels=16, out_channels=num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, graph_data.num_nodes, batch_size):\n",
    "        batch_mask = torch.arange(i, min(i + batch_size, graph_data.num_nodes), device=device)\n",
    "        batch_data = Data(\n",
    "            x=graph_data.x[batch_mask],\n",
    "            edge_index=graph_data.edge_index,\n",
    "            y=graph_data.y[batch_mask]\n",
    "        ).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch_data)\n",
    "        loss = criterion(out, batch_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_mask = torch.randperm(graph_data.num_nodes)[:1000].to(device)\n",
    "        val_data = Data(\n",
    "            x=graph_data.x[val_mask],\n",
    "            edge_index=graph_data.edge_index,\n",
    "            y=graph_data.y[val_mask]\n",
    "        ).to(device)\n",
    "        val_out = model(val_data)\n",
    "        val_loss = criterion(val_out, val_data.y)\n",
    "        val_pred = val_out.argmax(dim=1).cpu().numpy()\n",
    "        val_true = val_data.y.cpu().numpy()\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "    model.train()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c2818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Processing ../dataset/data/Monday-WorkingHours.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Wednesday-workingHours.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n",
      "C:\\Users\\gech\\AppData\\Local\\Temp\\ipykernel_3120\\678598562.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  chunk[col].fillna(chunk[col].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (141536, 79)\n",
      "Feature matrix shape: torch.Size([141536, 19])\n",
      "Label tensor shape: torch.Size([141536])\n",
      "Graph created with 141536 nodes and 566330 edges\n",
      "Epoch 1/20, Loss: 1049.1195, Val Loss: 3.3755, Val Acc: 0.8120\n",
      "Epoch 2/20, Loss: 976.9966, Val Loss: 3.1597, Val Acc: 0.8070\n",
      "Epoch 3/20, Loss: 991.6417, Val Loss: 2.7330, Val Acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import to_undirected, subgraph\n",
    "import scipy.sparse as sp\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 1: Data Loading and Preprocessing\n",
    "data_files = [\n",
    "    '../dataset/data/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Wednesday-workingHours.pcap_ISCX.csv',\n",
    "    '../dataset/data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    '../dataset/data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    '../dataset/data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "chunk_size = 50000\n",
    "data_chunks = []\n",
    "for file in data_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    try:\n",
    "        for chunk in pd.read_csv(file, chunksize=chunk_size, low_memory=False):\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            for col in chunk.select_dtypes(include=[np.float64, np.int64]).columns:\n",
    "                chunk[col].fillna(chunk[col].median(), inplace=True)\n",
    "            attack_map = {\n",
    "                'BENIGN': 'BENIGN', 'DDoS': 'DDoS', 'DoS Hulk': 'DoS', 'DoS GoldenEye': 'DoS',\n",
    "                'DoS slowloris': 'DoS', 'DoS Slowhttptest': 'DoS', 'PortScan': 'Port Scan',\n",
    "                'FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force', 'Bot': 'Botnet',\n",
    "                'Web Attack � Brute Force': 'Web Attack', 'Web Attack � XSS': 'Web Attack',\n",
    "                'Web Attack � Sql Injection': 'Web Attack', 'Infiltration': 'Infiltration',\n",
    "                'Heartbleed': 'Heartbleed'\n",
    "            }\n",
    "            chunk['Attack Type'] = chunk['Label'].map(attack_map)\n",
    "            chunk.drop('Label', axis=1, inplace=True)\n",
    "            chunk = chunk.sample(frac=0.05, random_state=42)\n",
    "            data_chunks.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "data = pd.concat(data_chunks, axis=0)\n",
    "print(f\"Final dataset shape: {data.shape}\")\n",
    "del data_chunks\n",
    "gc.collect()\n",
    "\n",
    "# Step 2: Feature Selection and Encoding\n",
    "features = [\n",
    "    'Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
    "    'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
    "    'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min',\n",
    "    'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "features = [f for f in features if f in data.columns]\n",
    "X = data[features]\n",
    "y = data['Attack Type']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long).to(device)\n",
    "print(f\"Feature matrix shape: {X_tensor.shape}\")\n",
    "print(f\"Label tensor shape: {y_tensor.shape}\")\n",
    "\n",
    "# Free memory\n",
    "del data, X, y, X_scaled, y_encoded\n",
    "gc.collect()\n",
    "\n",
    "# Step 3: Graph Construction\n",
    "n_samples = X_tensor.shape[0]\n",
    "k = 3\n",
    "adj_matrix = kneighbors_graph(X_tensor.cpu().numpy(), n_neighbors=k, mode='connectivity', include_self=False)\n",
    "adj_matrix = adj_matrix.tocoo()\n",
    "row = torch.tensor(adj_matrix.row, dtype=torch.long)\n",
    "col = torch.tensor(adj_matrix.col, dtype=torch.long)\n",
    "edge_index = torch.stack([row, col], dim=0)\n",
    "edge_index = to_undirected(edge_index)\n",
    "edge_index = edge_index.to(device)\n",
    "graph_data = Data(x=X_tensor, edge_index=edge_index, y=y_tensor).to(device)\n",
    "print(f\"Graph created with {graph_data.num_nodes} nodes and {graph_data.num_edges} edges\")\n",
    "\n",
    "# Step 4: Define GAT Model\n",
    "class GATNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):\n",
    "        super(GATNetwork, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.2)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)  # Node-level predictions, no global pooling\n",
    "        return x\n",
    "\n",
    "# Step 5: Training\n",
    "num_features = X_tensor.shape[1]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = GATNetwork(in_channels=num_features, hidden_channels=16, out_channels=num_classes).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, graph_data.num_nodes, batch_size):\n",
    "        batch_mask = torch.arange(i, min(i + batch_size, graph_data.num_nodes), device=device)\n",
    "        batch_edge_index, _ = subgraph(batch_mask, graph_data.edge_index, relabel_nodes=True, num_nodes=graph_data.num_nodes)\n",
    "        batch_data = Data(\n",
    "            x=graph_data.x[batch_mask],\n",
    "            edge_index=batch_edge_index,\n",
    "            y=graph_data.y[batch_mask]\n",
    "        ).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch_data)\n",
    "        loss = criterion(out, batch_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_mask = torch.randperm(graph_data.num_nodes)[:1000].to(device)\n",
    "        val_edge_index, _ = subgraph(val_mask, graph_data.edge_index, relabel_nodes=True, num_nodes=graph_data.num_nodes)\n",
    "        val_data = Data(\n",
    "            x=graph_data.x[val_mask],\n",
    "            edge_index=val_edge_index,\n",
    "            y=graph_data.y[val_mask]\n",
    "        ).to(device)\n",
    "        val_out = model(val_data)\n",
    "        val_loss = criterion(val_out, val_data.y)\n",
    "        val_pred = val_out.argmax(dim=1).cpu().numpy()\n",
    "        val_true = val_data.y.cpu().numpy()\n",
    "        val_acc = accuracy_score(val_true, val_pred)\n",
    "    model.train()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Use a subset for final evaluation to save memory\n",
    "    eval_mask = torch.randperm(graph_data.num_nodes)[:10000].to(device)\n",
    "    eval_edge_index, _ = subgraph(eval_mask, graph_data.edge_index, relabel_nodes=True, num_nodes=graph_data.num_nodes)\n",
    "    eval_data = Data(\n",
    "        x=graph_data.x[eval_mask],\n",
    "        edge_index=eval_edge_index,\n",
    "        y=graph_data.y[eval_mask]\n",
    "    ).to(device)\n",
    "    out = model(eval_data)\n",
    "    pred = out.argmax(dim=1).cpu().numpy()\n",
    "    true = eval_data.y.cpu().numpy()\n",
    "    acc = accuracy_score(true, pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(true, pred, average='weighted')\n",
    "    print(f\"Final Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e57ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
